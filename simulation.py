# %%
import os
os.path

# %%
# Import libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy import linalg as LA
from sklearn.datasets import make_sparse_spd_matrix
from utils.covest import CovEstWithNetwork
from utils.adpt_correlation_threshold import AdptCorrThreshold
from importlib import reload
from wlpy.gist import heatmap
rng = np.random.RandomState(19260817)  # we specify a random seed for replication

# %% Randomly generate S:population variance, and G observation matrix
N = 500
tau = 0.8
# S = make_spd_matrix(N, random_state=5)
S = make_sparse_spd_matrix(N, random_state=100)
G = ((S - np.diag(np.diag(S))) > tau) * 1

param_list = {
    'Number of Assets:': N, 
    'Generating Method': 'Randomly generate sparse p.d. matrix',
    'Observing threshold:' : tau,   
} 
# df = pd.DataFrame(param_list) 
print((~(S == 0)).sum())
print(G.sum())
print(param_list)
# %% <====== Bicke and Levina ======>
# Same setting as in BL2008 cov regularization
# The model is AR(1)
# N = 400
# rho = 0.7
# S = np.zeros(shape=[N, N])
# for j in range(0, N):
#     S = S + np.diag(np.ones(N-j)*(rho**j), -j) + \
#         np.diag(np.ones(N-j)*(rho**j), j)
# G = (S >= 0.49) * 1
# %% <====== Blocked Bicke and Levina ======>
def gen_S(rho = 0.8,N= 500):
    N = 500
    S_block = np.zeros(shape=[N, N])
    for j in range(0, N):
        S_block = S_block + np.diag(np.ones(N-j)*(rho**j), -j) + \
        np.diag(np.ones(N-j)*(rho**j), j)
    S = S_block - np.eye(N)
    return S

S = gen_S(rho = 0.8, N=500)
"""
G generated by mean + normal noise
"""
def gen_G(S, scale):
    G = rng.normal(S.reshape(-1), scale=scale).reshape(N,N)
    G= 0.5* (G + G.transpose())
    G =G-np.diag(np.diag(G))
    return G
G = gen_G(S, 0.2)

def gen_G2(S, l):
    G2 = ((S - np.diag(np.diag(S))) > tau) * 1
    return G2
G2 = gen_G2(S, l = 0.7)
# <------------>
# %% Generate Random obs
def generate_sample(S, T = 200):
    # rng = np.random.RandomState(100)
    N = S.shape[0]
    X1 = rng.multivariate_normal(mean=np.zeros(N), cov=S, size=T)
    return X1
X1 = generate_sample(S, 200)

# %% Estimate the cov
def estimate(G, X1):
    m = AdptCorrThreshold(pd.DataFrame(X1), G)
    b = m.find_smallest_threshold_for_pd()
    params = m.params_by_cv('pd', b)
    S_new = m.fit_adaptive_corr_threshold(params)   
    return m, S_new, params
m, S_new, params = estimate(G, X1)
print(params)


def dd_rslt(S, m, norm_type = 'fro'):
    dd = {"S": LA.norm(S),
        "Sample Cov": LA.norm(m.sample_cov() - S, ord=norm_type),
        "Linear Shrinkage": LA.norm(m.lw_lin_shrink() - S, ord=norm_type),
        "Nonlinear Shrinkage": LA.norm(m.nonlin_shrink() - S, ord=norm_type)}
    return dd

# %% Result
norm_type = 1
print('Norm', norm_type)
print(LA.norm(S, ord=norm_type))
print(LA.norm(m.sample_cov() - S, ord=norm_type))
print(LA.norm(m.lw_lin_shrink() - S, ord=norm_type))
print(LA.norm(m.nonlin_shrink() - S, ord=norm_type))
print(LA.norm(S_new - S, ord=norm_type))
# %%


def print_rslt(norm_type='fro'):
    print('Norm:', norm_type)
    print(LA.norm(S, ord=norm_type))
    print(LA.norm(m.sample_cov() - S, ord=norm_type))
    print(LA.norm(m.lw_lin_shrink() - S, ord=norm_type))
    print(LA.norm(m.nonlin_shrink() - S, ord=norm_type))
    # print(LA.norm(m.ha - S, ord=norm_type))
    print(LA.norm(S_new - S, ord=norm_type))

print_rslt()



# %%
# -> Created on 21 November 2020
# -> Author: Weiguang Liu
# %%
heatmap(S)
heatmap(S_new)
heatmap(m.sample_cov())
heatmap(m.lw_lin_shrink())
heatmap(m.nonlin_shrink())
# %%
# [estimate(G, generate_sample(S)) for i in range(100)]

# %%
S1 = m.fit_adaptive_corr_threshold(params = [2,0])
heatmap(S1)
# %%
rslt = []
for i in range(1):
    for rho in [0.8, 0.9, 0.95, 0.99]:
        for scale in np.linspace(0,0.4,5):
            S = gen_S(rho, N = 500)
            G = gen_G(S, scale)
            X1 = generate_sample(S, T = 200)
            m, S_new, params = estimate(G, X1)
            dct = dd_rslt(S, m, 1)
            dct["Adapt Corr Thresholding"] = LA.norm(S_new - S)
            dct["rho"] = rho
            dct["scale"] = scale
            rslt+= [dct]
            # print(rho)
# %%
rslt2 = []
for i in range(1):
    for rho in [0.8, 0.9, 0.95, 0.99]:
        for l in [0.5, 0.6, 0.7, 0.8, 0.9]:
            S = gen_S(rho, N=500)
            G2 = gen_G2(S, l)
            X1 = generate_sample(S, T=200)
            m, S_new, params = estimate(G2, X1)
            dct = dd_rslt(S, m, 1)
            dct["Adapt Corr Thresholding"] = LA.norm(S_new - S)
            dct["rho"] = rho
            dct["l"] = l
            rslt2 += [dct]

# %%
df = pd.DataFrame(rslt)
pd.set_option("precision", 2)
df = df.set_index(['rho', 'scale'])
with open('rslt.json', 'w') as f:
    f.write(df.to_latex())
# %%
df
# %%
